# Predicting Image Compressability
Project to predict image compressibility of climate data from CESM-LE using classification methods in R

![Edge Detection Example](https://github.com/andyreetz/Predicting-Image-Compressability/blob/main/edge%20detection%20example%20image.PNG)

## Introduction
We work to predict the compressibility of images generated by the National Center for Atmospheric Research’s (NCAR) Community Earth System Model - Large Ensemble (CESM-LE). This model produces spatial data for hundreds of climate variables that may be of interest to climate scientists. Due to the expense of storing this massive amount of data, it is desirable to compress the data without degrading the results of the scientific analysis for which the data is used. With a brute-force approach, optimal compression levels were obtained for the training and validation observations, but this method is not feasible for the massive amount of data generated by CESM-LE. Therefore, our objective is to engineer features that can can use to train and develop a high-performing classification model to predict the optimal compression level of these climate images.

### Task 1 Overview - Feature Construction
In order to predict the optimal compression levels, we must first construct and engineer features from the raw CESM-LE data. Because the data could represent almost any climate variable we need to develop features from the raw data instead of the provided variables in the training and validation observations. We work to build a feature space of both simple and complex predictors computed from both global and local statistics that can be used to train a high-performing predictive model.

### Task 2 Overview - Model Development
To create a model capable of accurate predictions we examine several methods. Because many of the engineered
features initially are highly non-normal we either normalize them, or use methods that do not assume feature
normality.

## Conclusions
We implemented a wide range of classification models including K-Nearest Neighbors (KNN), Support Vector Classifier (SVC), and Random Forest to asses our initial feature creation and benchmark accuracy. Although some of the features exhibited interesting trends, the excessive time to create some of these features led us to exclude them and focus on features that would appear to have more significance at lower cost. Once the features were engineered, we implemented a majority-vote ensemble model using KNN, Boosted Trees, and SVC together to predict image compression classification, which achieved a 91% classification accuracy on the validation set. Surprisingly, global statistics such as signal to noise ratio showed high importance for predicting the image compression. Adding localized gradients improved accuracy, but these features were not as significant as anticipated. This might mean the the spatial granularity of the image is not a significant factor in its compress-ability. Although our group’s final ensemble model achieved a 91% classification accuracy on the validation set, this does not mean that the model is highly interpretable. As long as classification accuracy is the primary concern, one potential route forward would be to train additional models to participate in the voting process, such as LDA or QDA. Although our current models are reasonably resistant to non-normality, some scaling of the data is required (as in KNN). Including LDA or QDA in the ensemble would require more deeply assessing the effects of different data normalizing and/or scaling procedures on the bias and variance of our model’s predictions. Additionally, some of the engineered features are highly correlated, such as the Range and IQR of edge detection values, which may lead to multicollinearlity and decrease model interpretability or accuracy. Future work in this area could involve removing unhelpful or mostly redundant features.

![Feature Importance of Engineered Features](https://github.com/andyreetz/Predicting-Image-Compressability/blob/main/edge%20detect%20full%20feature%20importance.PNG)


| Feature | Feature Importance |
| :--- | :--- |
| snrsoft30 | 24.179 |
| snr | 12.358 |
| pca_sum_2 | 11.945 |
| iqr_sobel | 6.989 |
| zeroes_sobel | 5.841 |
| imgmean | 5.682 |
| pca_sum_1 | 3.985 |
| snrsoft20 | 3.776 |
| shannonentropy | 3.349 |
| snrsoft10 | 2.686 |
